Bom dia, eu tive esse erro aqui no momento em que fui converter o dataframe em parquet no windows: "Py4JJavaError: An error occurred while calling [...] Writing job aborted". Além do winutils.exe conforme o professor indicou nas aulas (para versão 3.3.0 peguei do github https://github.com/kontext-tech/winutils/blob/master/hadoop-3.3.0/bin), para consertar esse erro, eu tive que adicionar o arquivo hadoop.dll nos binários do spark